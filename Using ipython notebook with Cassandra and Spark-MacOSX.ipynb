{
 "metadata": {
  "name": "",
  "signature": "sha256:e25b17660c7a15172c237d351c374e5faf222cf1425406f125df39e6762398be"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Installing ipython-notebook"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I assume that you already have python installed. I'm using python 2.7 for this project. \n",
      "\n",
      "To install ipython, you can use the following install command:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo pip install ipython"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You will need pip to install other modules. It is included in python 2.7.9, so it may be easier to just download or upgrade your python installation. If that is not an option, download get-pip.py here: https://bootstrap.pypa.io/get-pip.py and run with administrator privileges:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "python get-pip.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another way to install pip is with easy_install, if you happen to have that installed:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo easy_install pip"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ipython does have a CLI in addition to the notebook interface. However, you likely want to install the notebook code, as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo pip install \"ipython[notebook]\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You may discover, as I did, that you also need to install gnureadline."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo pip install gnureadline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can also install nose, a tool for running tests. I also found that I needed to install setuptools."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo pip install nose\n",
      "sudo pip install setuptools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And run a test to see if ipython notebook is set up correctly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iptest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using ipython-cql"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you wish to do queries from Cassandra, install the excellent ipython-cql module that Jon Haddad has created  (https://github.com/rustyrazorblade/ipython-cql)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo pip install ipython-cql"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once you have installed ipython-cql, you can select keyspaces and tables and run CQL commands. You will need to have a Cassandra or DSE instance running. These instructions assume a local instance on the same computer as ipython notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext cql"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/cassandra_driver-2.1.2-py2.7-macosx-10.9-intel.egg/cassandra/util.py:360: UserWarning: The blist library is not available, so a pure python list-based set will be used in place of blist.sortedset for set collection values. You can find the blist library here: https://pypi.python.org/pypi/blist/\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%keyspace test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using keyspace test\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%tables"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<table>\n",
        "    <tr>\n",
        "        <th>name</th>\n",
        "        <th>partition key</th>\n",
        "        <th>clustering key</th>\n",
        "        <th>compaction</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>kv</td>\n",
        "        <td>key</td>\n",
        "        <td></td>\n",
        "        <td>SizeTieredCompactionStrategy</td>\n",
        "    </tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "+------+---------------+----------------+------------------------------+\n",
        "| name | partition key | clustering key |          compaction          |\n",
        "+------+---------------+----------------+------------------------------+\n",
        "|  kv  |      key      |                | SizeTieredCompactionStrategy |\n",
        "+------+---------------+----------------+------------------------------+"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cql select * from kv;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<table>\n",
        "    <tr>\n",
        "        <th>key</th>\n",
        "        <th>value</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>1</td>\n",
        "        <td>abc</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>2</td>\n",
        "        <td>def</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>3</td>\n",
        "        <td>ghi</td>\n",
        "    </tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "+-----+-------+\n",
        "| key | value |\n",
        "+-----+-------+\n",
        "|  1  |  abc  |\n",
        "|  2  |  def  |\n",
        "|  3  |  ghi  |\n",
        "+-----+-------+"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These commands are useful if you just want to see what data you have in the tables before doing analytics."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Running pyspark against DSE "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Change the default setting for the number of cores per application in ~/dse/resources/spark/conf/spark-env.sh in to \"1\". Otherwise, the Spark shell that you'll start will use all the cores, preventing the ipython pyspark code from running. Find the line with <b>export DEFAULT_PER_APP_CORES</b> and change to <b>export DEFAULT_PER_APP_CORES=\"1\"</b>."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You'll need to start DSE with Spark using a command like \"sudo ./dse cassandra -k\" from the dse install location."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, if you want to be able to check out how Spark is running, start Spark shell to get localhost:7080 for spark://127.0.0.1:7077. The Spark shell will not be used for any input/output, only to monitor the Spark jobs run within the ipython notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo ./dse spark"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The ipython notebook will use pyspark to run the spark jobs. Check to see if pyspark code works first, to avoid frustration around getting pyspark to work with the ipython notebook. This next bit of code comes from the DataStax documentation. Create a directory called \"example\". Within \"example\", create a directory called \"connector\". Now, create a file called <b>__init__.py</b> with the following code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os \n",
      "from os import getenv \n",
      "from os.path import join \n",
      "import sys \n",
      "from subprocess import check_output \n",
      "\n",
      "HOME = getenv(\"HOME\") \n",
      "DSE_HOME = getenv(\"DSE_HOME\",join(HOME,\"dse\"))\n",
      "SPARK_HOME = join(DSE_HOME,\"resources\",\"spark\") \n",
      "os.environ['SPARK_HOME']=SPARK_HOME\n",
      "\n",
      "PYSPARK_DIR = join(DSE_HOME,\"resources\",\"spark\",\"python\") \n",
      "ADD_PATH = [ PYSPARK_DIR ] \n",
      "for PATH in ADD_PATH: \n",
      "  if PATH not in sys.path: \n",
      "    sys.path.insert(1,PATH)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: there is currently an error in two lines in the DataStax docs for this file. The code above is correct. Also, I have DSE installed at ~/dse in this example."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a file called <b>connector.py</b> in \"example/connector\" with the following code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# connector.py\n",
      "\n",
      "from pyspark import SparkContext, SparkConf\n",
      "\n",
      "def getSC():\n",
      "        conf = SparkConf().setAppName(\"Stand Alone Python Script\")\n",
      "        sc = SparkContext(conf=conf)\n",
      "        return sc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, create a file called <b>moduleexample.py</b> in \"example\" using the following code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "from connector.connector import getSC\n",
      "\n",
      "sc = getSC()\n",
      "print sc.cassandraTable(\"test\", \"kv\").collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obviously, the keyspace <b>test</b> and table <b>kv</b> should have some data in them. I'm using the same table that I showed above in the ipython-cql section."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run this program at the command line, to check that the pyspark connector is working with DSE. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " python example/moduleexample.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you run into issues, it is most likely that you have to add some environment variables to include the path to the python, spark, and dse run paths. To illustrate, I give you a portion of my <b>~/.bash_profile</b> file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "export DSE_HOME=/Users/lorinapoland/dse\n",
      "export SPARK_HOME=$DSE_HOME/resources/spark\n",
      "export PYSPARK_DIR=$DSE_HOME/resources/spark/python/pyspark\n",
      "\n",
      "export PYSPARK_SUBMIT_ARGS=\"--master local[2]\"\n",
      "\n",
      "export PATH=$PATH:$DSE_HOME/bin/:$DSE_HOME:$SPARK_HOME:$PYSPARK_DIR:"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using DSE and pyspark with ipython notebook"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to use Spark in the ipython notebook,  after DSE-Spark is installed and working, create a new IPython profile for PySpark. An excellent bit of code that I have borrowed from for this instruction is found at http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ipython profile create pyspark"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To avoid port conflicts with other IPython profiles, I updated the default port to 42424 within ~/.ipython/profile_pyspark/ipython_notebook_config.py:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = get_config()\n",
      "\n",
      "# Simply find this line and change the port value\n",
      "c.NotebookApp.port = 42424"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You will note in the .bash_profile above that I set two Spark environment variables, one for <b>SPARK_HOME</b> and one for <b>PYSPARK_SUBMIT_ARGS</b>."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a file named ~/.ipython/profile_pyspark/startup/00-pyspark-setup.py containing the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# configure the necessary Spark environment\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "spark_home = os.environ.get('SPARK_HOME',None)\n",
      "sys.path.insert(0,spark_home + \"/python\")\n",
      "\n",
      "# add py4j to the path\n",
      "sys.path.insert(0,os.path.join(spark_home, 'python/py4j'))\n",
      "\n",
      "# initialize pyspark to predefine the SparkContext variable 'sc'\n",
      "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, run the following command to start ipython notebook with pyspark from the directory ~/dse/resources/spark:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo IPYTHON_OPTS=\"notebook --pylab inline \u2014-profile=pyspark\u201c ./bin/pyspark"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You will likely now have a terminal tab running the Scala Spark shell, and one running ipython notebook."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then you're ready to try out some code! Below, I just pull out some data from a Cassandra table that I previously populated, just as I did with the standalone pyspark program moduleexample.py. Here, I run the code from within ipython notebook, and see the results below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "# get the spark path\n",
      "os.environ['SPARK_HOME'] = '/Users/lorinapoland/dse/resources/spark/'\n",
      "\n",
      "# and python path\n",
      "import sys\n",
      "sys.path.insert(0, '/Users/lorinapoland/dse/resources/spark/python')\n",
      "# identify the spark machine\n",
      "CLUSTER = 'spark://127.0.0.1:7077'\n",
      "print CLUSTER\n",
      "\n",
      "from pyspark import SparkContext\n",
      "# You can set the app name to anything; this just affects what\n",
      "# will show up in the UI.\n",
      "app_name = \"LLP-app\"\n",
      "sc = SparkContext(CLUSTER, app_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "spark://127.0.0.1:7077\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "15/02/17 13:10:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that I print the <b>CLUSTER</b> name to be sure I'm talking to the spark cluster. I have named the application \"LLP-app\" so that I can check its running status in the Spark browser window I opened earlier. Recall that I can check this out at http://localhost:7080/. Now, both the Spark shell and LLP-app should be running, each with one core."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext, SparkConf\n",
      "\n",
      "print sc.cassandraTable(\"test\", \"kv\").collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Row(key=3, value=u'ghi'), Row(key=1, value=u'abc'), Row(key=2, value=u'def')]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that you can assign the return value from the spark collect to a list, in this case called \"junk\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext, SparkConf\n",
      "junk = sc.cassandraTable(\"test\",\"kv\").collect()\n",
      "print junk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Row(key=3, value=u'ghi'), Row(key=1, value=u'abc'), Row(key=2, value=u'def')]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that you can break out the tuple into separate lists."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list1, list2 = zip(*junk)\n",
      "print list1\n",
      "print list2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3, 1, 2)\n",
        "(u'ghi', u'abc', u'def')\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
