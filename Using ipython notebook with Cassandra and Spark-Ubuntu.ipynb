{
 "metadata": {
  "name": "",
  "signature": "sha256:0fb1148e28a3a64ff950695fe3355ac34d4723b9ed155e63c69757b332e2d454"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These instructions start with a brand spanking new Ubuntu VM. Pick up where necessary, depending on what you already have installed."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Create Ubuntu VM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I created a new Ubuntu 12.04 VM with 4 GB RAM, 4 cores, and 16 GB of disk. Multiple cores are a good idea, so that you can run both the pyspark shell and ipython notebook with a spark app at the same time."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Install Java"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Install Java from Oracle download. I used Java 8 64-bit, but DataStax recommends Java 7. </p>\n",
      "<p>Here is the URL: http://www.oracle.com/technetwork/java/javase/downloads/index.html</p>\n",
      "<p>You'll need to add some environment variable settings to your .bash_profile (or .bashrc if you prefer):</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    export JAVA_HOME=~/jdk1.8.0_40\n",
      "    export PATH=$PATH:$JAVA_HOME"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use update-alternatives to create a link to the newly installed Java. I've installed Java in my home directory in this example, and link /usr/bin/java to that location."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    sudo update-alternatives --install \"/usr/bin/java\" \"java\" \"/home/polandll/jdk1.8.0_40/bin/java\" 1"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Install DSE Analytics (Spark)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intall DSE with Spark using the installer you can download from DataStax. You'll need to register in order to download it. Use the default settings and choose \"Spark Only\". I chose to install as Services+Utilities. I used 127.0.0.1 as the IP address, so that I could run it locally."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    sudo ~/Downloads/DataStaxEnterprise-4.6.1.2015021910-linux-x64-installer.run"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a couple of values to edit in configuration files. The first item to change is to uncomment the line that contains the following text in /etc/dse/spark/spark-env.sh, so that more than one spark app can run at a time."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    export $DEFAULT_PER_APP_CORES=\"1\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Change the seeds,listen_address, and rpc_address to 127.0.0.1 in /etc/dse/cassandra/cassandra.yaml."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    - seeds: \"127.0.0.1:\n",
      "    listen_address: 127.0.0.1\n",
      "    rpc_address: 127.0.0.1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start DSE. I chose to start it from /etc/init.d."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    cd /etc/init.d\n",
      "    sudo ./dse start"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start the pyspark shell and use your browser to check that the shell is running with the URL 127.0.0.1:7080. You should see a running app called \"PySparkShell\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    sudo dse pyspark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once you've checked that things are running correctly, you can shutdown the shell if you wish by typing quit()."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Installing ipython-notebook"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "python 2.7.3 will be pre-installed in Ubuntu. To simplify matters, use the synaptic package manager to install ipython. First, you'll need to install synaptic package manager by searching for the package in the Ubuntu Software Center."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the synaptic package manager is installed, start it up and search for and \"Mark for Installation\" the following packages:\n",
      "- ipython-notebook\n",
      "- python-matplotlib\n",
      "- python-scipy\n",
      "- python-sympy\n",
      "- python-pandas\n",
      "- python-setuptools\n",
      "\n",
      "Some of these are installed mainly to get prerequisite packages. Installation will take a few minutes, as there are quite a few packages. Click the Apply button and go get a cup of tea."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a number of actions we'll take using pip, a python installer program. First, we need to get it."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    sudo easy_install pip"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then we need to upgrade it. Yes, we use pip to upgrade pip - weird, huh?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    sudo pip install --upgrade pip"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's upgrade the ipython packages that we downloaded with synaptic package manager."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    sudo pip install --upgrade ipython[all]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since you are a Cassandra/DSE user, you'll likely want the wonderful ipython-cql plug-in created by our very own Jon Haddad (https://github.com/rustyrazorblade/ipython-cql)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    sudo pip install ipython-cql"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Configuring ipython notebook"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a pyspark profile in ipython. You can call it anything, but pyspark will remind you that this is for pyspark. Thanks to this website for helpful hints: http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    ipython profile create pyspark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set the notebook location in ~/.ipython/profile_pyspark/ipython_notebook_config.py. My example shows storing the notebooks in a directory called \"ipython-notebooks\" in my homedir."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    c.NotebookApp.notebook_dir = u'/home/polandll/ipython-notebooks')"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a file, ~/.ipython/profile_pyspark/startup/00-pyspark-setup.py using the following code:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    # Configure the necessary Spark environment\n",
      "    import os\n",
      "    import sys\n",
      "    spark_home = os.environ.get('SPARK_HOME',None)\n",
      "      if not spark_home:\n",
      "        raise ValueError('SPARK_HOME environment variable is not set')\n",
      "    sys.path.insert(0,os.path,join(spark_home, '/python'))\n",
      "    # Add py4j to the path\n",
      "    sys.path.insert(0,os.path.join(spark_home, '/python/py4j'))\n",
      "    # Initialize pyspark to predefine the SparkContext variable 'sc'\n",
      "    execfile(os.path.join(spark_home, '/python/pyspark/shell.py'))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And edit your ~/.bash_profile to include the following environment variables:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    export DSE_HOME=/usr/share/dse\n",
      "    export SPARK_HOME=$DSE_HOME/resources/spark\n",
      "    export PYSPARK_DIR=$DSE_HOME/resources/spark/python/pyspark\n",
      "    \n",
      "    export PYSPARK_SUBMIT_ARGS=\"--master local[2]\"\n",
      "    \n",
      "    export PATH=$PATH:$DSE_HOME/bin:$DSE_HOME:$SPARK_HOME:$PYSPARK_DIR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You'll need to source ~/.bash_profile in order to use these environment variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    source ~/.bash_profile"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Create keyspace and table for sample data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you want to try out my simple examples below, create this data in Cassandra. Create a keyspace, a table, and insert some data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    CREATE KEYSPACE test WITH REPLICATION = {'class':'SimpleStrategy', 'replication_factor':1 }\n",
      "    CREATE TABLE people ( name text, age int, PRIMARY KEY (name,age));\n",
      "    INSERT INTO people (name, age) VALUES ('jane',55);\n",
      "    INSERT INTO people (name, age) VALUES ('john',25);\n",
      "    INSERT INTO people (name, age) VALUES ('mary',45);\n",
      "    INSERT INTO people (name, age) VALUES ('pippin',125);"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using ipython-cql"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start ipython notebook with the pyspark profile."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    sudo ipython notebook --profile=pyspark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If a browser window doesn't open, or you get an error message, go to 127.0.0.1:8888. Under \"Clusters\", start the pyspark cluster. Under \"Files\" use the pulldown to start a new notebook. Remember that you can check 127.0.0.1:7080 to see what is going on with all spark apps."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, you finally have all the software installed, and you can actually try using ipython notebook! This first bit of code loads ipython-cql and executes a command to use the keyspace \"test\" and show what tables are located in this keyspace."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext cql\n",
      "%keyspace test\n",
      "%tables"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using keyspace test\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/cassandra_driver-2.1.2-py2.7-macosx-10.9-intel.egg/cassandra/util.py:360: UserWarning: The blist library is not available, so a pure python list-based set will be used in place of blist.sortedset for set collection values. You can find the blist library here: https://pypi.python.org/pypi/blist/\n"
       ]
      },
      {
       "html": [
        "<table>\n",
        "    <tr>\n",
        "        <th>name</th>\n",
        "        <th>partition key</th>\n",
        "        <th>clustering key</th>\n",
        "        <th>compaction</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>survey</td>\n",
        "        <td>submit_time</td>\n",
        "        <td>location</td>\n",
        "        <td>SizeTieredCompactionStrategy</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>kv</td>\n",
        "        <td>key</td>\n",
        "        <td></td>\n",
        "        <td>SizeTieredCompactionStrategy</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>people</td>\n",
        "        <td>name</td>\n",
        "        <td>age</td>\n",
        "        <td>SizeTieredCompactionStrategy</td>\n",
        "    </tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "+--------+---------------+----------------+------------------------------+\n",
        "|  name  | partition key | clustering key |          compaction          |\n",
        "+--------+---------------+----------------+------------------------------+\n",
        "| survey |  submit_time  |    location    | SizeTieredCompactionStrategy |\n",
        "|   kv   |      key      |                | SizeTieredCompactionStrategy |\n",
        "| people |      name     |      age       | SizeTieredCompactionStrategy |\n",
        "+--------+---------------+----------------+------------------------------+"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, you can query the table \"people\" for values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cql select * from people;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<table>\n",
        "    <tr>\n",
        "        <th>name</th>\n",
        "        <th>age</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>pippin</td>\n",
        "        <td>125</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>billy</td>\n",
        "        <td>45</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>jane</td>\n",
        "        <td>55</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>john</td>\n",
        "        <td>25</td>\n",
        "    </tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "+--------+-----+\n",
        "|  name  | age |\n",
        "+--------+-----+\n",
        "| pippin | 125 |\n",
        "| billy  |  45 |\n",
        "|  jane  |  55 |\n",
        "|  john  |  25 |\n",
        "+--------+-----+"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These commands are useful if you just want to see what data you have in the tables before doing analytics."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using DSE and pyspark with ipython notebook"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This example uses the SparkContext 'sc' to get data from the table \"people\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "# get the spark path\n",
      "os.environ['SPARK_HOME'] = '/usr/share/dse/resources/spark/'\n",
      "\n",
      "# and python path\n",
      "import sys\n",
      "sys.path.insert(0, '/usr/share/dse/resources/spark/python')\n",
      "\n",
      "from pyspark import SparkContext\n",
      "# PROFILE PYSPARK SHOULD START A SPARKCONTEXT BUT DOESN'T SEEM TO\n",
      "# identify the spark machine\n",
      "CLUSTER = 'spark://127.0.0.1:7077'\n",
      "# You can set the app name to anything; this just affects what\n",
      "# will show up in the UI.\n",
      "app_name = \"TestApp\"\n",
      "# Get a spark context\n",
      "sc = SparkContext(CLUSTER, app_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "spark://127.0.0.1:7077\n"
       ]
      },
      {
       "ename": "Exception",
       "evalue": "Launching GatewayServer failed with exit code 2!\nWarning: Expected GatewayServer to output a port, but found the following:\n\n--------------------------------------------------------------\nUnable to start spark: SPARK_MASTER not found\n--------------------------------------------------------------\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-adfb44ebff26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# will show up in the UI.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mapp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LLP-app\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLUSTER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/lorinapoland/dse/resources/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mtempNamedTuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Callsite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"function file linenum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempNamedTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinenum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
        "\u001b[0;32m/Users/lorinapoland/dse/resources/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writeToFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteToFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/lorinapoland/dse/resources/spark/python/pyspark/java_gateway.pyc\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgateway_port\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"--------------------------------------------------------------\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# In Windows, ensure the Java child processes do not linger after Python has exited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mException\u001b[0m: Launching GatewayServer failed with exit code 2!\nWarning: Expected GatewayServer to output a port, but found the following:\n\n--------------------------------------------------------------\nUnable to start spark: SPARK_MASTER not found\n--------------------------------------------------------------\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After a spark context is available, data from the table people is accessed and printed. Note that the results are assigned to the variable \"results\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext, SparkConf\n",
      "\n",
      "results = sc.cassandraTable(\"test\", \"people\").collect()\n",
      "print results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'sc' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-df4fb2df89fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcassandraTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"people\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Matplotlib and Graphs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can break out the tuple into separate lists and use the resulting information to create a simple graph. This is accomplished using matplotlib, which we installed as a ipython plug-in."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list1, list2 = zip(*results)\n",
      "print list1\n",
      "print list2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3, 1, 2)\n",
        "(u'ghi', u'abc', u'def')\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The matplotlib must be loaded. Plotting libraries are imported."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now a plot of the values in list1 can be created."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(list1)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "SPARK RDD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This next example is again simple, but shows that Cassandra data can be grabbed, printed, and a count of the rows can be done. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create the following table and insert the data shown."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    CREATE TABLE survey (\n",
      "      submit_time text,\n",
      "      location text,\n",
      "      added_comments text,\n",
      "      added_value int,\n",
      "      cass_exp int,\n",
      "      class_mgmt int,\n",
      "      db_exp int,\n",
      "      demos int,\n",
      "      email text,\n",
      "      encouraging int,\n",
      "      enough_handson int,\n",
      "      exercises int,\n",
      "      had_exp int,  \n",
      "      instructor text,\n",
      "      instructor_comment text,\n",
      "      knowledge int,\n",
      "      materials_comment text,\n",
      "      met_needs int,\n",
      "      name text,\n",
      "      not_valuable text,\n",
      "      nps int,\n",
      "      personal_interact int,\n",
      "      personal_presence int,\n",
      "      phone text,\n",
      "      prepared int,\n",
      "      role text,\n",
      "      sequenced int,\n",
      "      solr_exp int,\n",
      "      source text,\n",
      "      spark_exp int,\n",
      "      start_date text,\n",
      "      uptodate int,\n",
      "      use_feedback text,\n",
      "      valuable text,\n",
      "      PRIMARY KEY ((submit_time), location));"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    INSERT INTO survey (CREATE TABLE survey (submit_time,location, added_comments,added_value,cass_exp,class_mgmt,db_exp,demos,email,encouraging,enough_handson,exercises,had_exp,instructor,instructor_comment,knowledge,materials_comment,met_needs,name,not_valuable,nps,personal_interact,personal_presence,phone,prepared,role,sequenced,solr_exp,source,spark_exp,start_date,uptodate,use_feedback,valuable) VALUES (Feb 17 2015 - 4:56pm,RWC,Nope,3,1,4,1,2,,2,2,3,0,JoeC,Nice guy. ,1,Good stuff. ,3,,Hing,5,3,3,,2,DevOps,4,1,EventBrite,2,Feb 20 2015,0,N,Dm)\n",
      "    INSERT INTO survey (CREATE TABLE survey (submit_time,location, added_comments,added_value,cass_exp,class_mgmt,db_exp,demos,email,encouraging,enough_handson,exercises,had_exp,instructor,instructor_comment,knowledge,materials_comment,met_needs,name,not_valuable,nps,personal_interact,personal_presence,phone,prepared,role,sequenced,solr_exp,source,spark_exp,start_date,uptodate,use_feedback,valuable) VALUES (Feb 18 2015 - 4:56pm,RWC,Nope,3,1,4,1,2,,2,2,3,0,JoeC,Happy Guy. ,1,Brilliant. ,3,,Hing,5,3,3,,2,SE,4,1,EventBrite,2,Feb 20 2015,0,N,Dm)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext, SparkConf\n",
      "survey_array = sc.cassandraTable(\"test\", \"survey\").collect()\n",
      "print survey_array\n",
      "rdd1 = sc.parallelize(survey_array)\n",
      "type(rdd1)\n",
      "firstRow = rdd1.first()\n",
      "print \"HERE IS THE FIRST ROW\"\n",
      "print firstRow\n",
      "firstCount = rdd1.count()\n",
      "print \"HERE IS THE COUNT OF THE TABLE\"\n",
      "print firstCount"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}